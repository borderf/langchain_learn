from langchain.chat_models import init_chat_model
from langchain_classic.chains.question_answering.map_reduce_prompt import messages
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

model = init_chat_model(
    model="Qwen/Qwen3-8B",
    model_provider="openai",
    temperature=0.5,

)

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Python ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿è§£é‡ŠæŠ€æœ¯æ¦‚å¿µ")
]
print("å¤šè½®å¯¹è¯ï¼ˆè¾“å…¥'quit'é€€å‡ºï¼‰")
while True:
    user_input = input("ğŸ˜„ä½ ï¼š")
    if user_input == "quit":
        break
    # ä¿å­˜ç”¨æˆ·ä¿¡æ¯
    messages.append(HumanMessage(content=user_input))

    # è°ƒç”¨æ¨¡å‹
    response = model.invoke(messages)

    # è®°å½•AIçš„å›å¤
    messages.append(AIMessage(content=response.content))

    print(f"ğŸ¤– AIï¼š{response.content}")